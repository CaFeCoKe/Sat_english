{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sat_pre_training.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"13JryeVuK09SgoBAtxl5HKO8uIKtIABlW","authorship_tag":"ABX9TyPMY4x6oVMqh9aKS8yXwR1K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 데이터 준비"],"metadata":{"id":"sAtovljrSzv8"}},{"cell_type":"code","source":["import nltk\n","\n","nltk.download(\"punkt\")\n","from nltk.tokenize import word_tokenize\n","\n","from torchtext.legacy.data import Field\n","from torchtext.legacy.data import TabularDataset\n","from torchtext.legacy.data import BucketIterator\n","from torchtext.legacy.data import Iterator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Burr1ejRJr_Q","executionInfo":{"status":"ok","timestamp":1647416794794,"user_tz":-540,"elapsed":8336,"user":{"displayName":"CaFe CoKe","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKQbV_6mqL_VHCcbuNL4GHNUxGOJQBtuNsOFo3=s64","userId":"11886396836022920274"}},"outputId":"91ebc263-5120-4738-862a-494d40dcc910"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbBR3MuyF7VY"},"outputs":[],"source":["DATA_PATH = \"/content/drive/Othercomputers/내 컴퓨터/Sat_english/data/processed\""]},{"cell_type":"markdown","source":["필드 정의"],"metadata":{"id":"QsXwSJdPX5iN"}},{"cell_type":"code","source":["# 문장 필드\n","TEXT = Field(\n","    sequential=True,    # 문장 입력\n","    use_vocab=True,     \n","    tokenize=word_tokenize,     # nltk의 word_tokenize로 트큰화\n","    lower=True,         # 모두 소문자 처리\n","    batch_first=True,\n",")\n","\n","# 정답 필드\n","LABEL = Field(\n","    sequential=False,\n","    use_vocab=False,\n","    batch_first=True,\n",")"],"metadata":{"id":"lfxKZjPrTBOz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터 불러오기"],"metadata":{"id":"LValW3OmX-31"}},{"cell_type":"code","source":["# CoLA 데이터 = 사전 학습 데이터\n","cola_train_data, cola_valid_data, cola_test_data = TabularDataset.splits(\n","    path=DATA_PATH,\n","    train=\"cola_train.tsv\",\n","    validation=\"cola_valid.tsv\",\n","    test=\"cola_test.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1,          # column명이 있는 1열 생략\n",")\n","\n","TEXT.build_vocab(cola_train_data, min_freq=2)   # CoLA 데이터로 사전학습할 단어장 생성(2번 이상 나온 단어만)\n","\n","# 수능 데이터 = 추가 학습 데이터\n","sat_train_data, sat_valid_data, sat_test_data = TabularDataset.splits(\n","    path=DATA_PATH,\n","    train=\"sat_train.tsv\",\n","    validation=\"sat_valid.tsv\",\n","    test=\"sat_test.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1,\n",")"],"metadata":{"id":"ggRQO7KgT4Bk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DataLoader 정의"],"metadata":{"id":"4CdaR71bYBRP"}},{"cell_type":"code","source":["# CoLA 데이터\n","cola_train_iterator, cola_valid_iterator, cola_test_iterator = BucketIterator.splits(\n","    (cola_train_data, cola_valid_data, cola_test_data),\n","    batch_size=32,\n","    device=None,\n","    sort=False,\n",")\n","\n","# 수능 데이터\n","sat_train_iterator, sat_valid_iterator, sat_test_iterator = BucketIterator.splits(\n","    (sat_train_data, sat_valid_data, sat_test_data),\n","    batch_size=8,\n","    device=None,\n","    sort=False,\n",")"],"metadata":{"id":"SEY6mJmCX1Fj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 네트워크 구성"],"metadata":{"id":"niRzsFTqXpF-"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"723vR_q6X7o8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LSTM_Model(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, pad_idx):\n","        super().__init__()\n","\n","        # Embedding Layer\n","        self.embed_layer = nn.Embedding(\n","            num_embeddings=num_embeddings, \n","            embedding_dim=embedding_dim, \n","            padding_idx=pad_idx\n","        )\n","\n","        # LSTM Layer\n","        self.lstm_layer = nn.LSTM(\n","            input_size=embedding_dim, \n","            hidden_size=hidden_size, \n","            num_layers=num_layers, \n","            bidirectional=True,     # 양방향 LSTM\n","            dropout=0.5\n","        )\n","\n","        # Fully-connetcted Layer\n","        self.fc_layer1 = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),    # 양방향 LSTM의 출력은 입력의 2배\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU()      # f(x)=max(0.01x, x)로 dying ReLU 방지\n","        )\n","        self.fc_layer2 = nn.Sequential(\n","            nn.Linear(hidden_size, 1),\n","            nn.Sigmoid()        # 확률 출력을 위함\n","        )\n","\n","    def forward(self, x):\n","        embed_x = self.embed_layer(x)\n","\n","        output, (_, _) = self.lstm_layer(embed_x)       # hidden, cell state의 출력값 사용 안함\n","        \n","        print(f'재배치 전: {output.size()}')\n","        # output = output.view(-1, hidden_size*2) \n","        output = output[:, -1, :]\n","        print(f'재배치 후: {output.size()}')\n","\n","        output = self.fc_layer1(output)\n","        output = self.fc_layer2(output)\n","        return output"],"metadata":{"id":"iVAvvIEbX0IX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 학습 및 검증"],"metadata":{"id":"_GvTG58LuI9g"}},{"cell_type":"code","source":["USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")    # GPU 존재시 GPU 실행(CUDA)\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]       # 동일한 크기를 맞추기 위한 패딩문자를 숫자 식별자에 매칭 -> 숫자 식별자=index\n","\n","lstm = LSTM_Model(\n","        num_embeddings=len(TEXT.vocab),\n","        embedding_dim=100,\n","        hidden_size=200,\n","        num_layers=4,\n","        pad_idx=PAD_IDX\n","    ).to(DEVICE)\n","\n","epochs = 20\n","learning_rate = 0.001\n","\n","optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n","criterion = nn.BCELoss()"],"metadata":{"id":"Mqy27QLCutiw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터로 학습하여 모델화"],"metadata":{"id":"jsONag9Zw1Xm"}},{"cell_type":"code","source":["def train(model, iterator, optimizer):\n","    train_loss = 0\n","\n","    model.train()       # 모델을 train모드로 설정(Dropout 적용)\n","    for batch in enumerate(iterator):\n","        optimizer.zero_grad()   # optimizer 초기화(Gradient)\n","\n","        text = batch.text      # 해당 Batch의 text 속성 불러오기\n","        label = batch.label.type(torch.FloatTensor)     # 해당 Batch의 label 속성 불러오기(32-bit float)\n","\n","        text = text.to(DEVICE)\n","        label = label.to(DEVICE)\n","\n","        output = model(text)\n","        loss = criterion(output, label)\n","        loss.backward()     # 역전파로 Gradient를 계산 후 파라미터에 할당\n","        optimizer.step()    # 파라미터 업데이트\n","\n","\n","        train_loss += loss.item()   # Loss 값 누적\n","\n","    return train_loss / len(iterator)       # Loss 값을 Batch 값으로 나누어 미니 배치마다의 Loss 값의 평균을 구함"],"metadata":{"id":"RmCedQivuJNd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 검증"],"metadata":{"id":"KITUxPSfuLjc"}},{"cell_type":"code","source":["def evaluate(model, iterator):\n","    valid_loss = 0\n","\n","    model.eval()        # 모델을 eval모드로 설정(Dropout 미적용)\n","    with torch.no_grad():       # Gradient 계산 비활성화 (모델 평가에는 파라미터 업데이트 X)\n","        for  batch in enumerate(iterator):\n","            text = batch.text\n","            label = batch.label.type(torch.FloatTensor)\n","\n","            text = text.to(DEVICE)\n","            label = label.to(DEVICE)\n","            \n","            output = model(text)\n","            loss = criterion(output, label)\n","\n","            valid_loss += loss.item()\n","\n","    return valid_loss / len(iterator)"],"metadata":{"id":"rnvLNdwKuLyV"},"execution_count":null,"outputs":[]}]}